{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78cda806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'src') \n",
    "sys.path.insert(0, '/data/ch52669/gene_interaction/tot-gene-sets/MedAgents') \n",
    "import os\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2770bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "50\n",
      "50\n",
      "29\n",
      "51\n",
      "29\n",
      "100\n",
      "50\n",
      "50\n",
      "29\n",
      "50\n",
      "30\n",
      "30\n",
      "30\n",
      "42\n",
      "62\n",
      "100\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "data_size = 100\n",
    "with open('results2/tot_eval_1_gen_1_voter_5steps.pkl', 'rb') as f:\n",
    "    tot_results_ori = pickle.load(f)\n",
    "    tot_results_ori = tot_results_ori[:data_size]\n",
    "    print(len(tot_results_ori))\n",
    "    \n",
    "with open('results2/tot_eval_uncertainty_5steps_abla_uncertain_False.pkl', 'rb') as f:\n",
    "    tot_results_gpt3_abla = pickle.load(f)\n",
    "    tot_results_gpt3_abla = tot_results_gpt3_abla[:data_size]\n",
    "    print(len(tot_results_gpt3_abla))\n",
    "    \n",
    "with open('results2/tot_eval_uncertainty_5steps_abla_uncertain_False_th08.pkl', 'rb') as f:\n",
    "    tot_results_gpt3_abla08 = pickle.load(f)\n",
    "    tot_results_gpt3_abla08 = tot_results_gpt3_abla08[:data_size]\n",
    "    print(len(tot_results_gpt3_abla08))\n",
    "    \n",
    "with open('results2/tot_eval_uncertainty_5steps_abla_uncertain_False_th05.pkl', 'rb') as f:\n",
    "    tot_results_gpt3_abla05 = pickle.load(f)\n",
    "    tot_results_gpt3_abla05 = tot_results_gpt3_abla05[:data_size]\n",
    "    print(len(tot_results_gpt3_abla05))\n",
    "    \n",
    "with open('results2/tot_eval_uncertainty_5steps_explore_stop_expand.pkl', 'rb') as f:\n",
    "    tot_results_gpt3_explore_stop_expand = pickle.load(f)\n",
    "    tot_results_gpt3_explore_stop_expand = tot_results_gpt3_explore_stop_expand[:data_size]\n",
    "    print(len(tot_results_gpt3_explore_stop_expand))\n",
    "    \n",
    "with open('results2/tot_eval_uncertainty_5steps_abla_uncertain_False_th098.pkl', 'rb') as f:\n",
    "    tot_results_gpt3_abla098 = pickle.load(f)\n",
    "    tot_results_gpt3_abla098 = tot_results_gpt3_abla098[:data_size]\n",
    "    print(len(tot_results_gpt3_abla098))\n",
    "    \n",
    "with open('results2/tot_eval_uncertainty_5steps_explore.pkl', 'rb') as f:\n",
    "    tot_results_gpt3_explore = pickle.load(f)\n",
    "    tot_results_gpt3_explore = tot_results_gpt3_explore[:data_size]\n",
    "    print(len(tot_results_gpt3_explore))\n",
    "    \n",
    "with open('results2/tot_eval_uncertainty_5steps_3.pkl', 'rb') as f:\n",
    "    tot_results_gpt3 = pickle.load(f)\n",
    "    tot_results_gpt3 = tot_results_gpt3[:data_size]\n",
    "    print(len(tot_results_gpt3))\n",
    "    \n",
    "with open('results2/tot_eval_uncertainty_5steps_gpt4.pkl', 'rb') as f:\n",
    "    tot_results_gpt4 = pickle.load(f)\n",
    "    tot_results_gpt4 = tot_results_gpt4[:data_size]\n",
    "    print(len(tot_results_gpt4))\n",
    "\n",
    "with open('results2/tot_eval_uncertainty_5steps_abla_uncertain_15gen_False_th09.pkl', 'rb') as f:\n",
    "    tot_results_abla_uncertain_False_15gen_th09 = pickle.load(f)\n",
    "    tot_results_abla_uncertain_False_15gen_th09 = tot_results_abla_uncertain_False_15gen_th09[:data_size]\n",
    "    print(len(tot_results_abla_uncertain_False_15gen_th09))\n",
    "\n",
    "with open('results2/tot_eval_uncertainty_5steps_15gen_th09.pkl', 'rb') as f:\n",
    "    tot_results_abla_15gen_th09 = pickle.load(f)\n",
    "    tot_results_abla_15gen_th09 = tot_results_abla_15gen_th09[:data_size]\n",
    "    print(len(tot_results_abla_15gen_th09))\n",
    "    \n",
    "with open('results2/tot_eval_uncertainty_5steps_abla_uncertain_15gen_False_th09_9nodes.pkl', 'rb') as f:\n",
    "    tot_results_abla_uncertain_False_15gen_False_th09_9nodes = pickle.load(f)\n",
    "    tot_results_abla_uncertain_False_15gen_False_th09_9nodes = tot_results_abla_uncertain_False_15gen_False_th09_9nodes[:data_size]\n",
    "    print(len(tot_results_abla_uncertain_False_15gen_th09))\n",
    "    \n",
    "# with open('results2/tot_eval_uncertainty_5steps_stop_expand.pkl', 'rb') as f:\n",
    "#     tot_results_5steps_stop_expand = pickle.load(f)\n",
    "#     tot_results_5steps_stop_expand = tot_results_5steps_stop_expand[:data_size]\n",
    "#     print(len(tot_results_5steps_stop_expand))    \n",
    "\n",
    "with open('results2/tot2_eval_5steps_no_certainty_stop_expand.pkl', 'rb') as f:\n",
    "    tot2_eval_5steps_no_certainty_stop_expand = pickle.load(f)\n",
    "    tot2_eval_5steps_no_certainty_stop_expand = tot2_eval_5steps_no_certainty_stop_expand[:data_size]\n",
    "    print(len(tot2_eval_5steps_no_certainty_stop_expand))  \n",
    "with open('results2/tot2_eval_5steps_no_certainty_9nodes.pkl', 'rb') as f:\n",
    "    tot2_eval_5steps_no_certainty_9nodes = pickle.load(f)\n",
    "    tot2_eval_5steps_no_certainty_9nodes = tot2_eval_5steps_no_certainty_9nodes[:5] + tot2_eval_5steps_no_certainty_9nodes[6:24] + tot2_eval_5steps_no_certainty_9nodes[25:]\n",
    "    tot2_eval_5steps_no_certainty_9nodes = tot2_eval_5steps_no_certainty_9nodes[:data_size]\n",
    "    print(len(tot2_eval_5steps_no_certainty_9nodes))  \n",
    "with open('results2/tot2_eval_uncertainty_5steps_stop_expand.pkl', 'rb') as f:\n",
    "    tot2_eval_uncertainty_5steps_stop_expand = pickle.load(f)\n",
    "    tot2_eval_uncertainty_5steps_stop_expand = tot2_eval_uncertainty_5steps_stop_expand[:data_size]\n",
    "    print(len(tot2_eval_uncertainty_5steps_stop_expand))  \n",
    "    \n",
    "with open('results2/tot2_eval_uncertainty_4steps_th09.pkl', 'rb') as f:\n",
    "    tot2_eval_uncertainty_4steps_th09 = pickle.load(f)\n",
    "    tot2_eval_uncertainty_4steps_th09 = tot2_eval_uncertainty_4steps_th09[:data_size]\n",
    "    print(len(tot2_eval_uncertainty_4steps_th09))  \n",
    "    \n",
    "with open('results2/tot2_eval_uncertainty_5steps_th09.pkl', 'rb') as f:\n",
    "    tot2_eval_uncertainty_5steps_th09 = pickle.load(f)\n",
    "    tot2_eval_uncertainty_5steps_th09 = tot2_eval_uncertainty_5steps_th09[:data_size]\n",
    "    print(len(tot2_eval_uncertainty_5steps_th09))  \n",
    "\n",
    "with open('results2/tot2_eval_uncertainty_5steps_gpt4_9nodes_th09.pkl', 'rb') as f:\n",
    "    tot2_eval_uncertainty_5steps_gpt4_9nodes_th09 = pickle.load(f)\n",
    "    tot2_eval_uncertainty_5steps_gpt4_9nodes_th09 = tot2_eval_uncertainty_5steps_gpt4_9nodes_th09[:data_size]\n",
    "    print(len(tot2_eval_uncertainty_5steps_gpt4_9nodes_th09))  \n",
    "\n",
    "medagent_results = []\n",
    "with open(f'medagents_results/eval_medagents_gprofiler_step3_5.pkl', 'rb') as f:\n",
    "    text_file = pickle.load(f)\n",
    "with open(f'medagents_results/eval_medagents_gprofiler_step3_29.pkl', 'rb') as f:\n",
    "    text_file2 = pickle.load(f)\n",
    "medagent_results = text_file + text_file2\n",
    "medagent_results = medagent_results[:data_size]\n",
    "\n",
    "# for i in range(0, len(medagent_results)):\n",
    "print(len(medagent_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf07c477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "for i in tot2_eval_uncertainty_5steps_th09:\n",
    "    print(i['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f190b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One run GPT-3.5\n",
      "Best - Average: 0.6704, Average Quantile: 0.9788\n",
      "Final - Average: 0.4331, Average Quantile: 0.8126\n",
      "\n",
      "Five run GPT-3.5\n",
      "Best - Average: 0.6640, Average Quantile: 0.9667\n",
      "Final - Average: 0.4480, Average Quantile: 0.8144\n",
      "\n",
      "Five run and uncertainty GPT-3.5\n",
      "Best - Average: 0.6634, Average Quantile: 0.9624\n",
      "Final - Average: 0.4274, Average Quantile: 0.8088\n",
      "\n",
      "Five run and uncertainty GPT-4\n",
      "Best - Average: 0.6832, Average Quantile: 0.9706\n",
      "Final - Average: 0.4660, Average Quantile: 0.7990\n",
      "\n",
      "medagent GPT-4 (30 data samples only)\n",
      "Best - Average: 0.6466, Average Quantile: 0.9533\n",
      "Final - Average: 0.4968, Average Quantile: 0.8338\n",
      "\n",
      "Five run GPT-3.5 - th 0.5\n",
      "Best - Average: 0.6090, Average Quantile: 0.9453\n",
      "Final - Average: 0.4269, Average Quantile: 0.7632\n",
      "\n",
      "Five run GPT-3.5 - th 0.8\n",
      "Best - Average: 0.6370, Average Quantile: 0.9708\n",
      "Final - Average: 0.4006, Average Quantile: 0.7519\n",
      "\n",
      "Five run GPT-3.5 - th 0.98\n",
      "Best - Average: 0.6417, Average Quantile: 0.9704\n",
      "Final - Average: 0.4073, Average Quantile: 0.7730\n",
      "\n",
      "Five run and uncertainty metrics GPT-3.5 explore\n",
      "Best - Average: 0.6327, Average Quantile: 0.9489\n",
      "Final - Average: 0.3908, Average Quantile: 0.7418\n",
      "\n",
      "Five run and uncertainty metrics GPT-3.5 explore and stop_expand\n",
      "Best - Average: 0.4517, Average Quantile: 0.7194\n",
      "Final - Average: 0.3419, Average Quantile: 0.5912\n",
      "\n",
      "Fifteen run GPT-3.5 - th09\n",
      "Best - Average: 0.5925, Average Quantile: 0.9577\n",
      "Final - Average: 0.3990, Average Quantile: 0.7304\n",
      "\n",
      "Fifteen run GPT-3.5 and uncertainty - th09\n",
      "Best - Average: 0.5791, Average Quantile: 0.9338\n",
      "Final - Average: 0.4135, Average Quantile: 0.7534\n",
      "\n",
      "Fifteen run GPT-3.5 and uncertainty and nine nodes - th09\n",
      "Best - Average: 0.5944, Average Quantile: 0.9444\n",
      "Final - Average: 0.3705, Average Quantile: 0.6465\n",
      "\n",
      "tot2_eval_5steps_no_certainty_stop_expand\n",
      "Best - Average: 0.5978, Average Quantile: 0.9099\n",
      "Final - Average: 0.4315, Average Quantile: 0.6704\n",
      "\n",
      "tot2_eval_5steps_no_certainty_9nodes\n",
      "Best - Average: 0.6310, Average Quantile: 0.9492\n",
      "Final - Average: 0.4324, Average Quantile: 0.7101\n",
      "\n",
      "tot2_eval_uncertainty_5steps_stop_expand\n",
      "Best - Average: 0.5825, Average Quantile: 0.9002\n",
      "Final - Average: 0.3892, Average Quantile: 0.6853\n",
      "\n",
      "tot2_eval_uncertainty_4steps_th09\n",
      "Best - Average: 0.6181, Average Quantile: 0.9551\n",
      "Final - Average: 0.4118, Average Quantile: 0.6943\n",
      "\n",
      "tot2_eval_uncertainty_5steps_th09\n",
      "Best - Average: 0.6705, Average Quantile: 0.9740\n",
      "Final - Average: 0.4534, Average Quantile: 0.7727\n",
      "\n",
      "tot2_eval_uncertainty_5steps_gpt4_th09\n",
      "Best - Average: 0.7173, Average Quantile: 0.9803\n",
      "Final - Average: 0.5212, Average Quantile: 0.8400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prediction_report(sims, quantile):\n",
    "    # Calculate averages\n",
    "    avg_sim = sum(sims) / len(sims)\n",
    "    avg_quan = sum(quantile) / len(quantile)\n",
    "\n",
    "    # Return calculated averages\n",
    "    return avg_sim, avg_quan\n",
    "\n",
    "# Iterate over the results\n",
    "for i, n in enumerate([tot_results_ori, tot_results_gpt3_abla, tot_results_gpt3, \\\n",
    "                       tot_results_gpt4, medagent_results, tot_results_gpt3_abla05, \\\n",
    "                       tot_results_gpt3_abla08, tot_results_gpt3_abla098, \\\n",
    "                       tot_results_gpt3_explore, tot_results_gpt3_explore_stop_expand,\\\n",
    "                       tot_results_abla_uncertain_False_15gen_th09, \\\n",
    "                       tot_results_abla_15gen_th09, tot_results_abla_uncertain_False_15gen_False_th09_9nodes,\\\n",
    "#                        tot_results_5steps_stop_expand, \n",
    "                       tot2_eval_5steps_no_certainty_stop_expand, \\\n",
    "                       tot2_eval_5steps_no_certainty_9nodes, tot2_eval_uncertainty_5steps_stop_expand,\\\n",
    "                       tot2_eval_uncertainty_4steps_th09, tot2_eval_uncertainty_5steps_th09, tot2_eval_uncertainty_5steps_gpt4_th09,\\\n",
    "                       \n",
    "                      \n",
    "                      ]):\n",
    "    # Define lists to store scores and quantiles\n",
    "    best_sim = []\n",
    "    final_sim = []\n",
    "    best_sim_quan = []\n",
    "    final_sim_quan = []\n",
    "\n",
    "    # Extract scores and quantiles\n",
    "    for r in n:\n",
    "        best_sim.append(r['best similarity score'])\n",
    "        final_sim.append(r['final answer similarity score'])\n",
    "        best_sim_quan.append(r['best candidate similarity quantile'])\n",
    "        final_sim_quan.append(r['final answer similarity quantile'])\n",
    "\n",
    "    # Configuration titles\n",
    "    config_titles = ['One run GPT-3.5', 'Five run GPT-3.5', \n",
    "                     'Five run and uncertainty GPT-3.5', \n",
    "                     'Five run and uncertainty GPT-4', 'medagent GPT-4 (30 data samples only)',\\\n",
    "                     'Five run GPT-3.5 - th 0.5', 'Five run GPT-3.5 - th 0.8', 'Five run GPT-3.5 - th 0.98',\\\n",
    "                    'Five run and uncertainty metrics GPT-3.5 explore', 'Five run and uncertainty metrics GPT-3.5 explore and stop_expand',\\\n",
    "                     'Fifteen run GPT-3.5 - th09',\\\n",
    "                     'Fifteen run GPT-3.5 and uncertainty - th09',\\\n",
    "                     'Fifteen run GPT-3.5 and uncertainty and nine nodes - th09',\n",
    "#                      'tot_results_5steps_stop_expand',\\\n",
    "                     'tot2_eval_5steps_no_certainty_stop_expand',\\\n",
    "                     'tot2_eval_5steps_no_certainty_9nodes',\\\n",
    "                     'tot2_eval_uncertainty_5steps_stop_expand',\n",
    "                     'tot2_eval_uncertainty_4steps_th09', 'tot2_eval_uncertainty_5steps_th09',\\\n",
    "                     'tot2_eval_uncertainty_5steps_gpt4_th09',\n",
    "                    ]\n",
    "    \n",
    "    # Print configuration title\n",
    "    print(f'{config_titles[i]}')\n",
    "    \n",
    "    # Calculate and print averages for best\n",
    "    best_avg_sim, best_avg_quan = prediction_report(best_sim, best_sim_quan)\n",
    "    print(f'Best - Average: {best_avg_sim:.4f}, Average Quantile: {best_avg_quan:.4f}')\n",
    "    \n",
    "    # Calculate and print averages for final\n",
    "    final_avg_sim, final_avg_quan = prediction_report(final_sim, final_sim_quan)\n",
    "    print(f'Final - Average: {final_avg_sim:.4f}, Average Quantile: {final_avg_quan:.4f}')\n",
    "    print()  # For spacing between configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ea7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accb164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
